# app.py
# -*- coding: utf-8 -*-
"""
Streamlit: ê³„ì•½ì„œ ìë™ ì´ìŠˆ ë§ˆí‚¹(ë…ì†Œì¡°í•­) ë¦¬ë·°ì–´ â€” Google Sheet(ë¹„ê³µê°œ, Secrets) ê¸°ë°˜
"""

from __future__ import annotations
import os, io, re, json, time, uuid, html, unicodedata
from dataclasses import dataclass
from typing import List, Dict, Any, Optional

import streamlit as st
from pypdf import PdfReader

# DOCX (optional)
try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE = True
except Exception:
    DOCX_AVAILABLE = False

# gspread + google-auth
try:
    import gspread
    from google.oauth2.service_account import Credentials
    GSHEETS_AVAILABLE = True
except Exception:
    GSHEETS_AVAILABLE = False

# OpenAI
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except Exception:
    OPENAI_AVAILABLE = False

# ---------------- Constants ----------------
MAX_CHARS = 200_000
DEFAULT_MODEL = "gpt-4o-mini"

# ---------------- Data types ----------------
@dataclass
class Clause:
    idx: int
    title: str
    text: str
    start: int
    end: int

# ---------------- File loaders ----------------
def extract_text_pdf(bio: io.BytesIO) -> str:
    reader = PdfReader(bio)
    return "\n\n".join(p.extract_text() or "" for p in reader.pages)

def extract_text_docx(bio: io.BytesIO) -> str:
    if not DOCX_AVAILABLE: return ""
    doc = DocxDocument(bio)
    return "\n".join(p.text for p in doc.paragraphs)

def load_text_from_file(upload) -> str:
    data = upload.read()
    name = upload.name.lower()
    if name.endswith(".pdf"): return extract_text_pdf(io.BytesIO(data))
    if name.endswith(".docx") and DOCX_AVAILABLE: return extract_text_docx(io.BytesIO(data))
    for enc in ("utf-8", "cp949", "euc-kr"):
        try: return data.decode(enc)
        except Exception: continue
    return data.decode("utf-8", errors="ignore")

# ---------------- Clause splitter ----------------
def split_into_clauses_kokr(text: str) -> List[Clause]:
    header_pat = re.compile(r"(?m)^(ì œ\s*\d+\s*ì¡°[^\n]*)")
    headers = [(m.start(), m.group(0).strip()) for m in header_pat.finditer(text)]
    if not headers: return [Clause(1, "ì „ì²´ ê³„ì•½ì„œ", text.strip(), 0, len(text))]
    headers.append((len(text), "__END__"))
    clauses: List[Clause] = []
    for i in range(len(headers) - 1):
        start, title = headers[i]
        end = headers[i + 1][0]
        body = text[start:end].strip()
        if not body: continue
        clauses.append(Clause(i + 1, title, body, start, end))
    return clauses

# ---------------- Google Sheet loader ----------------
def _normalize(s: str) -> str: return unicodedata.normalize("NFC", (s or "").strip()).lower()

def _open_worksheet_robust(sh, target_name: Optional[str]):
    if not target_name: return sh.sheet1
    try: return sh.worksheet(target_name)
    except Exception: pass
    for ws in sh.worksheets():
        if _normalize(ws.title) == _normalize(target_name): return ws
    return sh.sheet1

def _read_secrets_gcp_sa() -> Optional[Dict[str, Any]]:
    import json as _json
    if "gcp_sa" in st.secrets: return dict(st.secrets["gcp_sa"])
    raw = st.secrets.get("GDRIVE_SERVICE_ACCOUNT_JSON", "").strip()
    if raw:
        cfg = _json.loads(raw)
        if "\\n" in cfg.get("private_key",""): cfg["private_key"] = cfg["private_key"].replace("\\n","\n")
        return cfg
    return None

def load_issues_from_gsheet_private() -> List[Dict[str, Any]]:
    if not GSHEETS_AVAILABLE: raise RuntimeError("gspread / google-auth íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    cfg = _read_secrets_gcp_sa()
    if not cfg: st.error("Streamlit Secretsì— GCP ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."); return []
    sheet_id = st.secrets.get("GSHEET_ID", "").strip()
    ws_name  = (st.secrets.get("GSHEET_WORKSHEET", "ë…ì†Œì¡°í•­_ì˜ˆì‹œ")).strip()
    if not sheet_id: st.error("Streamlit Secretsì— Google Sheet IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."); return []
    scopes = ["https://www.googleapis.com/auth/spreadsheets.readonly", "https://www.googleapis.com/auth/drive.readonly"]
    creds = Credentials.from_service_account_info(cfg, scopes=scopes)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(sheet_id)
    ws = _open_worksheet_robust(sh, ws_name)
    rows = ws.get_all_values()
    issues = []
    if not rows: return issues
    header = [c.strip().lower() for c in rows[0]]
    start_idx = 1 if set(["id","title","definition"]).intersection(header) else 0
    for r in rows[start_idx:]:
        if len(r) < 3: continue
        a,b,c = r[0].strip(), r[1].strip(), r[2].strip()
        if not (a or b or c): continue
        issues.append({"id": a or str(uuid.uuid4()), "title": b or a or "(untitled)", "definition": c})
    return issues

# ---------------- LLM ----------------
class OpenAILLM:
    def __init__(self, api_key: Optional[str]=None):
        if not OPENAI_AVAILABLE: raise RuntimeError("openai íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        self.api_key = api_key or st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not self.api_key: st.error("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì…ë ¥í•´ì£¼ì„¸ìš”."); st.stop()
        self.client = OpenAI(api_key=self.api_key)

    def review(self, *, model:str, issue_id:str, issue_title:str, issue_definition:str, full_text:str, clauses: List[Clause]) -> Dict[str, Any]:
        payload_text = full_text[:MAX_CHARS]
        
        # ê° ì¡°í•­ì˜ ë²ˆí˜¸ì™€ ì œëª©ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
        clause_map_str = "\n".join([f"- ì¡°í•­ {c.idx}: \"{c.title}\"" for c in clauses])

        # --- âœ¨ [ìˆ˜ì •ëœ ë¶€ë¶„] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ëŒ€í­ ìˆ˜ì • ---
        system = (
            "You are a meticulous Korean legal assistant. Your primary goal is to find specific, problematic phrases in a contract based on a given definition of a toxic clause. "
            "You must respond in KOREAN. Return a STRICT JSON object.\n\n"
            "**CRITICAL INSTRUCTIONS:**\n"
            "1.  **Analyze the Contract:** Review the entire `CONTRACT` text provided by the user.\n"
            "2.  **Identify Clause Numbers:** Use the `CLAUSE_LIST` to determine the correct clause number (e.g., ì œ14ì¡° is clause 14).\n"
            "3.  **Find Specific Evidence:** If you find a toxic clause, you MUST pinpoint the **exact problematic sentence or phrase** from the contract. This is for highlighting.\n"
            "4.  **Explain the Risk:** Clearly explain WHY that specific phrase is a problem, linking it to the `ISSUE_DEFINITION`.\n"
            "5.  **JSON OUTPUT:** Your output MUST be a single JSON object with this exact schema:\n"
            "    {\n"
            f"      \"issue_id\": \"{issue_id}\",\n"
            f"      \"issue_title\": \"{issue_title}\",\n"
            "      \"found\": boolean, // `true` if found, otherwise `false`\n"
            "      \"explanation\": \"(Provide a clear, concise, and intuitive explanation in Korean. Start with an emoji like âš ï¸ or ğŸ¤”.)\",\n"
            "      \"clause_indices\": number[], // **IMPORTANT**: If `found` is true, this array CANNOT be empty. It MUST contain the number(s) of the clause(s) where the issue was found.\n"
            "      \"evidence_quotes\": string[] // **IMPORTANT**: If `found` is true, this array CANNOT be empty. It MUST contain the exact quote(s).\n"
            "    }\n"
        )
        
        user = (
            f"## ISSUE_DEFINITION (ê²€í† í•  ë…ì†Œ ì¡°í•­ ì •ì˜):\n{issue_definition}\n\n"
            f"## CLAUSE_LIST (ê³„ì•½ì„œì˜ ì¡°í•­ ëª©ë¡):\n{clause_map_str}\n\n"
            f"## CONTRACT (ì „ì²´ ê³„ì•½ì„œ ë‚´ìš©):\n{payload_text}"
        )
    
        try:
            resp = self.client.chat.completions.create(
                model=model, messages=[{"role":"system","content":system},{"role":"user","content":user}],
                response_format={"type":"json_object"}, temperature=0.0,
            )
            data = json.loads(resp.choices[0].message.content or "{}")
            # --- âœ¨ [ìˆ˜ì •ëœ ë¶€ë¶„] ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ---
            if data.get("found") and not data.get("clause_indices"):
                st.warning(f"'{issue_title}' ì´ìŠˆëŠ” ë°œê²¬ë˜ì—ˆìœ¼ë‚˜, ê´€ë ¨ ì¡°í•­ ë²ˆí˜¸ë¥¼ íŠ¹ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                data["clause_indices"] = [] # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
        except Exception as e:
            st.warning(f"'{issue_title}' ê²€í†  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            data = {"issue_id": issue_id, "found": False, "explanation": f"LLM í˜¸ì¶œ ì˜¤ë¥˜: {e}", "clause_indices": [], "evidence_quotes": []}
        
        data.setdefault("issue_id", issue_id); data.setdefault("issue_title", issue_title)
        return data

# ---------------- Highlight helper ----------------
def _normalize_for_matching(text: str) -> str:
    return re.sub(r'[\s\n\r]+', '', text).lower()

def highlight_text(text: str, quotes: List[str]) -> str:
    safe_text = html.escape(text)
    temp_text = safe_text
    
    for q in quotes:
        q = q.strip()
        if not q: continue
        
        escaped_q = html.escape(q)
        # ë„ì–´ì“°ê¸°, ì¤„ë°”ê¿ˆ ë“±ì„ ë¬´ì‹œí•˜ê³  ì¼ì¹˜í•˜ëŠ” ë¶€ë¶„ì„ ì°¾ê¸° ìœ„í•œ ì •ê·œì‹ íŒ¨í„´ ìƒì„±
        pattern = re.escape(escaped_q).replace(r'\ ', r'\s*').replace(r'\n', r'\s*')
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ì— ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë¶€ë¶„ ì°¾ê¸°
        matches = list(re.finditer(pattern, temp_text, re.IGNORECASE | re.DOTALL))
        
        for match in reversed(matches): # ë’¤ì—ì„œë¶€í„° êµì²´í•´ì•¼ ì¸ë±ìŠ¤ê°€ ê¼¬ì´ì§€ ì•ŠìŒ
            start, end = match.span()
            original_phrase = temp_text[start:end]
            temp_text = temp_text[:start] + f"<mark>{original_phrase}</mark>" + temp_text[end:]
            
    return temp_text

# ---------------- UI ----------------
st.set_page_config(page_title="ê³„ì•½ì„œ ì´ìŠˆ ë§ˆí‚¹ ë·°ì–´", layout="wide")
st.title("ğŸ“‘ ê³„ì•½ì„œ ìë™ ì´ìŠˆ ë§ˆí‚¹ & í•˜ì´ë¼ì´íŠ¸ ë·°ì–´")

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    model = st.text_input("ëª¨ë¸ ì´ë¦„", value=DEFAULT_MODEL)
    api_key = st.text_input("OpenAI API Key", type="password", help="í‚¤ë¥¼ ì…ë ¥í•˜ë©´ Secrets ì„¤ì •ë³´ë‹¤ ìš°ì„  ì ìš©ë©ë‹ˆë‹¤.")
    
uploaded = st.file_uploader("ê³„ì•½ì„œ ì—…ë¡œë“œ (PDF/DOCX/TXT/MD)", type=["pdf","docx","txt","md"])
if not uploaded: st.info("ë¶„ì„í•  ê³„ì•½ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."); st.stop()

raw_text = load_text_from_file(uploaded)
if not raw_text.strip(): st.error("íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); st.stop()

clauses = split_into_clauses_kokr(raw_text)

if st.button("ğŸ” ë¶„ì„ ì‹œì‘í•˜ê¸°", type="primary"):
    with st.spinner('ê³„ì•½ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
        try:
            issues_cfg = load_issues_from_gsheet_private()
        except Exception as e:
            st.exception(e); st.stop()
        if not issues_cfg:
            st.error("Google Sheetì—ì„œ ë…ì†Œ ì¡°í•­ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."); st.stop()

        llm = OpenAILLM(api_key=api_key)
        results = []
        for issue in issues_cfg:
            data = llm.review(
                model=model, issue_id=issue.get("id", str(uuid.uuid4())),
                issue_title=issue.get("title", "Untitled"),
                issue_definition=issue.get("definition", ""), full_text=raw_text,
                clauses=clauses # ì¡°í•­ ëª©ë¡ì„ LLMì— ì „ë‹¬
            )
            results.append(data)
        
        st.session_state['results'] = results
        st.success("ğŸ‰ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if 'results' in st.session_state:
    results = st.session_state['results']
    found_issues = [r for r in results if r.get("found")]
    
    st.markdown("---")
    if not found_issues:
        st.success("âœ… ê²€í†  ê²°ê³¼, ê³„ì•½ì„œì—ì„œ íŠ¹ë³„í•œ ë…ì†Œ ì¡°í•­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        st.error(f"ğŸš¨ ì´ {len(found_issues)}ê°œì˜ ì ì¬ì  ì´ìŠˆê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # --- âœ¨ [ìˆ˜ì •ëœ ë¶€ë¶„] ì¡°í•­ ë¯¸ì§€ì • ì´ìŠˆ ìƒë‹¨ì— í‘œì‹œ ---
    unassigned_issues = [r for r in found_issues if not r.get("clause_indices")]
    if unassigned_issues:
        st.subheader("âš ï¸ ì¡°í•­ ë¯¸ì§€ì • ì´ìŠˆ")
        st.warning("ì•„ë˜ ì´ìŠˆë“¤ì€ ê³„ì•½ì„œì—ì„œ ë°œê²¬ë˜ì—ˆìœ¼ë‚˜, íŠ¹ì • ì¡°í•­ê³¼ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        for issue in unassigned_issues:
            with st.container(border=True):
                with st.chat_message("assistant", avatar="ğŸ¤”"):
                    st.markdown(f"**{issue.get('issue_title')}**")
                    st.markdown(issue.get('explanation', ''))
                    quotes = issue.get("evidence_quotes", [])
                    if quotes:
                        st.markdown("**ê·¼ê±° ë¬¸ì¥:**")
                        for q in quotes:
                            st.markdown(f"> {q}")
        st.markdown("---")


    st.subheader("ğŸ“„ ê³„ì•½ì„œ ì¡°í•­ë³„ ê²€í†  ê²°ê³¼")
    for c in clauses:
        matched_issues = [r for r in found_issues if c.idx in r.get("clause_indices", [])]
        
        all_quotes = [q for issue in matched_issues for q in issue.get("evidence_quotes", [])]
        highlighted_text = highlight_text(c.text, all_quotes)
        
        with st.container(border=True):
            st.markdown(f"### {html.escape(c.title)}")
            st.markdown(f"<div style='white-space: pre-wrap; line-height: 1.7;'>{highlighted_text}</div>", unsafe_allow_html=True)
            
            if matched_issues:
                st.markdown("---")
                for issue in matched_issues:
                    with st.chat_message("assistant", avatar="âš ï¸"):
                        st.markdown(f"**{issue.get('issue_title')}**")
                        st.markdown(issue.get('explanation', ''))
